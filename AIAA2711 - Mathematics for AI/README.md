## Official Introduction

Grading: A+ to F
---
For the final grade, the course utilizes an absolute grading system as follows
---
A+: [100,96]; A: (96,92]; A-: (92:88];
B+: (88,84]; B: (84,80]; B-: (80:76];
C+: (76,72]; C: (72,68]; C-: (68:64];
D: (64:60];
F: (60,0].
---

This course aims to teach students the basic math concepts for Artificial Intelligence. 

**Key topics** include fundamental Linear Algebra (Matrix Calculations, Norms, Eigenvectors and Eigenvalues), Calculus (Derivative, Taylor series, Multivariate Calculus), and Probability Theory (Distributions, Statistics of Random Variables, Bayesâ€™ theorem). 

With these mathematical concepts, some **basic principles of numerical optimization and typical AI algorithms** (Gradient Descent, Maximum-likelihood, Regression, Least Square Estimation, Spectral Clustering, Matrix Decomposition, etc.) will also be introduced as examples to better relate math to AI. 

The approach of this course is specifically Artificial Intelligence application oriented, aiming to help students to quickly establish a fundamental mathematical knowledge structure for AI studies. 

Through this course, students will acquire the fundamental mathematical concepts required for AI, understand the connections between AI and mathematics, and get prepared to learn the mathematical principles, formulas, inductions, and relevant proofs for advanced AI algorithms.

## Syllabus

in-class(10%) + attendance(20%) + Final Examination(40%) + project demonstration(30%) = 100%
---

| Week | Content |
|------|---------|
| 1 | **Affine Space and its generation**: Group, Vector Space, Linear Combination, Spanning and Dimension of Space, Linear Mapping, Affine Space |
| 2 | **Analytic Geometry**: Norm, Inner Product, Positive Definite, Distance and Angle, Orthonormal Basis,  Inner Product of Function |
| 3 | **Analytic Geometry**: Projections of Vectors, Gram-Schmidt Orthogonalization, Rotation |
| 4 | **Matrix Decompositions**: Eigenvalue and Eigenvector, Cholesky Decomposition, Eigendecomposition, Diagonalization |
| 5 | **Matrix Decompositions**: Singular Value Decomposition, Matrix Approximation |
| 6 | **Vector Caculus**: Differentiation, Gradient, Linear Approximation, Multivariate Taylor Expansion |
| 7 | **Vector Caculus**: Grandients of Vector-Valued Functions and Matrices, Backpropagation, Automatic Differentiation |
| 8 | **Vector Caculus**: Grandients of Vector-Valued Functions and Matrices, Backpropagation, Automatic Differentiation |
| 9 | **Optimization**: Convex sets, Convex Function, Convex Optimization, Linear Programming, Quadratic Programming |
| 10 | **Fundamental Probability Theory**: Sum and Product Rule, Bayes' Theorem, Mean, Covariance, Statistical Independence |
| 11 | **Fundamental Probability Theory**:Single- and Multi-variables Gaussians Distribution,  Marginals and Conditionals of Gaussians, Sums and Linear Transformations of Gaussian, Change of Variables |
| 12~14 | ***Project Demo 1~3*** |
